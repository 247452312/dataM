神经网络:
    神经网络类似于大脑;
    ∑XiWi = W0 + X1W1 + X2W2 + X3W3 + X4W4 + ... + XnWn
    这里 X0一定为1,这样N维超平面才能上下平移

    神经网络每个感知基的运作过程 输入为(1,X1,X2,X3 .... Xn) 分别乘权重(W0,W1,W2,....,Wn)
    得到和, 和通过激励函数得到输出

    假设激励函数为Y=X 则 第i个输入的权重的误差为 学习率 * (期望输出 - 实际输出) * 第i个输入

    神经网络感知基激活函数:
        输出 = 1 / (1 + e^输入)  输出不会超过1也不会低于0(X为输入 o为输出)

        输入越接近0  倒数越大, 所以在训练时希望从导数大的位置开始训练 也就是将权重设置的比较小
    BP算法(误差逆传播算法)
        求误差对于某个权重的偏导
        只有一层感知基的时候求误差都是求 学习率 * ∑(期望输出-实际输出)^2 * X
        BP算法可以将之转换为求误差对第J个神经元的输入求偏导
        1.输出神经元求权重:
            误差对输入求偏导 = E对输出求偏导 * 输出对输入求偏导
            误差对输出求偏导 = 导((1/2) * ∑(期望输出 - 输出)^2)
                         = -(期望输出 - 输出)
            输出对输入求偏导 = 见上面神经网络激活函数(o = 1 / (1 + e^x)) 求导
                            = 输出(1-输出)

            综合来看 误差对输入求偏导 = 误差对输出求偏导 * 输出对输入求偏导
                                  = -(期望输出 - 输出) * 输出 * (1-输出)
                     误差即为 -学习率 * 误差对输入求偏导 * Xi
        BP算法最终公式:
            第X个权重的误差 = 学习率 * 输出对中间和求导 * ∑(下一层神经元的误差 * 下一层神经元对应此神经元的权重) * 第X个输入
                                            ↓
                                         1/(1 + e^和)
    神经网络过学习:
        分成训练集,校验集,验证集
        在训练集上重复学习,每次训练完毕后使用校验集校验,校验集校验的结果一开始会下降,后来会上升,从拐点停止学习
    冲量:
        学习时增加冲量, 也就是 n时刻的修改量要加入 n-1时刻的修改量 * 学习率2
    学习率:
        一般是0.05,0.1,0.2 不能太大和太小, 太小容易落入局部最优解,太大容易产生震荡,不能稳定收敛 所以学习率可以动态调整

    以上都叫BP网络

    其他:
        可以联想记忆神经的网络:
                Hopfield Network
                    ->修复有噪声的信息
        每个分类器都有好处
            神经网络训练时间比较长 但反应速度比较块 可解释性比较差
    文献资料:
        见图pic31
