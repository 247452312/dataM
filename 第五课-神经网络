神经网络:
    神经网络类似于大脑;
    感知基输入与输出:∑XiWi = W0 + X1W1 + X2W2 + X3W3 + X4W4 + ... + XnWn
    这里 X0一定为1,这样N维超平面才能上下平移
    神经网络是一个有监督式学习的分类器.

    神经网络的一个感知基:
        通过不断的修正W0,W1...Wn 来得到最优解
        方法:
            梯度下降法
        批处理模式:
            E(w) = (1/2) * ∑(期望 - 实际)^2  批处理数据集(也可以)    1/2求导用,没有实际意义
        知错就改模式:
            过程如图pic20 图中 是一个与非门, 即输入都是1时输出零,其他情况输出1 , 将这种情况转换为 结果大于0.5时输出1 结果小于0.5时输出0
            输入(X0,X1,X2)在(1,0,0),(1,0,1),(1,1,0),(1,1,1)轮训
            计算出期望值和实际输出值之后,调整每一个W的偏差 Wi = Wi + 0.1 * (期望 - 输出) * Xi
            直到收敛到偏差不变
        缺陷:
            不能解决线性不可分问题. 例如 13象限为+ 24象限为- 则无法训练出正确的感知基
    多层感知基:
        原本的感知基加了隐含层

        意义,不加隐含层时的分类不能解决线性不可分问题,即在平面内画一条线,分开两类数据,加入隐含层后就像加入了几条线,分开平面内的数据
        感知基的函数通常是 1 / (1 + e^-x)

        隐含层误差:
            见图pic21 BP误差逆传播算法

            第一步,计算最后一层的神经元的权重误差

    神经网络每个感知基的运作过程 输入为(1,X1,X2,X3 .... Xn) 分别乘权重(W0,W1,W2,....,Wn)
    得到和, 和通过激励函数得到输出

    假设激励函数为Y=X 则 第i个输入的权重的误差为 学习率 * (期望输出 - 实际输出) * 第i个输入

    神经网络感知基激活函数:
        输出 = 1 / (1 + e^输入)  输出不会超过1也不会低于0(X为输入 o为输出)

        输入越接近0  倒数越大, 所以在训练时希望从导数大的位置开始训练 也就是将权重设置的比较小
    BP算法(误差逆传播算法)
        求误差对于某个权重的偏导
        只有一层感知基的时候求误差都是求 学习率 * ∑(期望输出-实际输出)^2 * X
        BP算法可以将之转换为求误差对第J个神经元的输入求偏导
        1.输出神经元求权重:
            误差对输入求偏导 = E对输出求偏导 * 输出对输入求偏导
            误差对输出求偏导 = 导((1/2) * ∑(期望输出 - 输出)^2)
                         = -(期望输出 - 输出)
            输出对输入求偏导 = 见上面神经网络激活函数(o = 1 / (1 + e^x)) 求导
                            = 输出(1-输出)

            综合来看 误差对输入求偏导 = 误差对输出求偏导 * 输出对输入求偏导
                                  = -(期望输出 - 输出) * 输出 * (1-输出)
                     误差即为 -学习率 * 误差对输入求偏导 * Xi
        BP算法最终公式:
            第X个权重的误差 = 学习率 * 输出对中间和求导 * ∑(下一层神经元的误差 * 下一层神经元对应此神经元的权重) * 第X个输入
                                            ↓
                                         1/(1 + e^和)
    神经网络过学习:
        分成训练集,校验集,验证集
        在训练集上重复学习,每次训练完毕后使用校验集校验,校验集校验的结果一开始会下降,后来会上升,从拐点停止学习
    冲量:
        学习时增加冲量, 也就是 n时刻的修改量要加入 n-1时刻的修改量 * 学习率2
    学习率:
        一般是0.05,0.1,0.2 不能太大和太小, 太小容易落入局部最优解,太大容易产生震荡,不能稳定收敛 所以学习率可以动态调整

    以上都叫BP网络

    其他:
        可以联想记忆神经的网络:
                Hopfield Network
                    ->修复有噪声的信息
        每个分类器都有好处
            神经网络训练时间比较长 但反应速度比较块 可解释性比较差
    文献资料:
        见图pic31
