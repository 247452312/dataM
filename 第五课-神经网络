神经网络:
    神经网络类似于大脑;
    感知基输入与输出:∑XiWi = W0 + X1W1 + X2W2 + X3W3 + X4W4 + ... + XnWn
    这里 X0一定为1,这样N维超平面才能上下平移
    神经网络是一个有监督式学习的分类器.

    神经网络的一个感知基:
        通过不断的修正W0,W1...Wn 来得到最优解
        方法:
            梯度下降法
        批处理模式:
            E(w) = (1/2) * ∑(期望 - 实际)^2  批处理数据集(也可以)    1/2求导用,没有实际意义
        知错就改模式:
            过程如图pic20 图中 是一个与非门, 即输入都是1时输出零,其他情况输出1 , 将这种情况转换为 结果大于0.5时输出1 结果小于0.5时输出0
            输入(X0,X1,X2)在(1,0,0),(1,0,1),(1,1,0),(1,1,1)轮训
            计算出期望值和实际输出值之后,调整每一个W的偏差 Wi = Wi + 0.1 * (期望 - 输出) * Xi
            直到收敛到偏差不变
        缺陷:
            不能解决线性不可分问题. 例如 13象限为+ 24象限为- 则无法训练出正确的感知基
    多层感知基:
        原本的感知基加了隐含层

        意义,不加隐含层时的分类不能解决线性不可分问题,即在平面内画一条线,分开两类数据,加入隐含层后就像加入了几条线,分开平面内的数据
        感知基的函数通常是 1 / (1 + e^-x)

        隐含层误差:
            见图pic21 BP误差逆传播算法

            第一步,计算最后一层的神经元的权重误差